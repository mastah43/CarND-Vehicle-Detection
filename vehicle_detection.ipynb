{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vehicle Detection\n",
    "[![Udacity - Self-Driving Car NanoDegree](https://s3.amazonaws.com/udacity-sdc/github/shield-carnd.svg)](http://www.udacity.com/drive)\n",
    "\n",
    "This project implements a software pipeline to detect vehicles in a video.  \n",
    "\n",
    "The Project\n",
    "---\n",
    "\n",
    "The goals / steps of this project are the following:\n",
    "\n",
    "* Perform a Histogram of Oriented Gradients (HOG) feature extraction on a labeled training set of images and train a classifier Linear SVM classifier\n",
    "* Optionally, you can also apply a color transform and append binned color features, as well as histograms of color, to your HOG feature vector. \n",
    "* Note: for those first two steps don't forget to normalize your features and randomize a selection for training and testing.\n",
    "* Implement a sliding-window technique and use your trained classifier to search for vehicles in images.\n",
    "* Run your pipeline on a video stream (start with the test_video.mp4 and later implement on full project_video.mp4) and create a heat map of recurring detections frame by frame to reject outliers and follow detected vehicles.\n",
    "* Estimate a bounding box for vehicles detected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import importlib\n",
    "import numpy as np\n",
    "from sklearn import svm\n",
    "from skimage.feature import hog\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.ndimage.measurements import label\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import glob\n",
    "import itertools\n",
    "import math\n",
    "import collections\n",
    "from moviepy.editor import VideoFileClip\n",
    "from IPython.display import HTML\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "np.random.seed(12061979) # make random number generation repeatable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_images_grid(images, labels=[],axis_on=False):\n",
    "    grid_cols = 8 if len(images) >= 16 else 4 if len(images) >= 8 else 2 if len(images) >=2 else 1\n",
    "    \n",
    "    grid_rows = math.ceil(len(images)/float(grid_cols))\n",
    "    fig_height_inches = math.ceil((16/float(grid_cols))*grid_rows)\n",
    "    fig, axes = plt.subplots(grid_rows, grid_cols, figsize=(16, fig_height_inches))\n",
    "    fig.subplots_adjust(wspace=0.001, hspace=0.2)\n",
    "    \n",
    "    for ax, image, label in itertools.zip_longest(axes.ravel(), images, labels):\n",
    "        if image is not None:\n",
    "            ax.imshow(image)\n",
    "            if label is not None:\n",
    "                ax.set_title(label)\n",
    "        if not axis_on:\n",
    "            ax.axis('off')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helper for Image Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_img(file):\n",
    "    return cv2.cvtColor(cv2.imread(file), cv2.COLOR_BGR2RGB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Test Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import glob\n",
    "image_files = glob.glob('./test_images/*.jpg')\n",
    "imgs_test = []\n",
    "for img_file in image_files:\n",
    "    imgs_test.append(load_img(img_file))\n",
    "img_test = imgs_test[0]\n",
    "plt.imshow(img_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training Data Set\n",
    "Data set images are loaded into memory in order to speed up feature extraction necessary for\n",
    "e.g. hog parameter exploration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vehicle_img_files = glob.glob('training_dataset/vehicles/**/*.png')\n",
    "nonvehicle_img_files = glob.glob('training_dataset/non-vehicles/**/*.png')\n",
    "print(\"vehicles: \" + str(len(vehicle_img_files)), \", non vehicles: \" + str(len(nonvehicle_img_files)))\n",
    "\n",
    "vehicle_imgs = [load_img(file) for file in tqdm(vehicle_img_files)]\n",
    "nonvehicle_imgs = [load_img(file) for file in tqdm(nonvehicle_img_files)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of positive and negative training images in the data set is nearly equal and thus sufficiently balanced already - no need to add / remove samples for a category."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vehicle Images from Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_images_grid([vehicle_imgs[i] for i in np.random.randint(0,len(vehicle_imgs),(32))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Non Vehicle Images from Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_images_grid([nonvehicle_imgs[i] for i in np.random.randint(0,len(nonvehicle_imgs),(32))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HOG Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convert_color_from_rgb(img, color_space_dst):\n",
    "    if color_space_dst == 'RGB':\n",
    "        return img\n",
    "    elif color_space_dst == 'HSV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2HSV)\n",
    "    elif color_space_dst == 'LUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2LUV)\n",
    "    elif color_space_dst == 'HLS':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2HLS)\n",
    "    elif color_space_dst == 'YUV':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2YUV)\n",
    "    elif color_space_dst == 'YCrCb':\n",
    "        return cv2.cvtColor(img, cv2.COLOR_RGB2YCrCb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hog_features(img, orientation, pixels_per_cell, cells_per_block, visualise):\n",
    "    features, hog_image = hog(\n",
    "        img, \n",
    "        orientations=orientation, \n",
    "        pixels_per_cell=(pixels_per_cell, pixels_per_cell),\n",
    "        cells_per_block=(cells_per_block, cells_per_block), \n",
    "        transform_sqrt=True, \n",
    "        visualise=visualise, \n",
    "        feature_vector=True)\n",
    "    return features, hog_image\n",
    "\n",
    "def hog_visualize(img):\n",
    "    img_conv = convert_color_from_rgb(img, 'YCrCb')\n",
    "    _, img_hog = hog_features(img_conv[:,:,0], orientation=8, pixels_per_cell=8, \n",
    "                              cells_per_block=2, visualise=True)\n",
    "    return img_hog\n",
    "    \n",
    "vehicle_img = vehicle_imgs[0]\n",
    "vehicle_hog = hog_visualize(vehicle_img)\n",
    "\n",
    "nonvehicle_img = nonvehicle_imgs[1]\n",
    "nonvehicle_hog = hog_visualize(nonvehicle_img)\n",
    "\n",
    "# plot original and hog features for vehicle and non vehicle image\n",
    "f, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(7,7))\n",
    "f.subplots_adjust(hspace = .4, wspace=.2)\n",
    "ax1.imshow(vehicle_img)\n",
    "ax1.set_title('vehicle image', fontsize=16)\n",
    "ax2.imshow(vehicle_hog, cmap='gray')\n",
    "ax2.set_title('vehicle hog', fontsize=16)\n",
    "ax3.imshow(nonvehicle_img)\n",
    "ax3.set_title('non vehicle image', fontsize=16)\n",
    "ax4.imshow(nonvehicle_hog, cmap='gray')\n",
    "ax4.set_title('non vehicle hog', fontsize=16)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spatial bins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bin_spatial(img, size=(32, 32)):\n",
    "    color1 = cv2.resize(img[:,:,0], size).ravel()\n",
    "    color2 = cv2.resize(img[:,:,1], size).ravel()\n",
    "    color3 = cv2.resize(img[:,:,2], size).ravel()\n",
    "    return np.hstack((color1, color2, color3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Color Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def color_hist(img, nbins=32):    #bins_range=(0, 256)\n",
    "    # Compute the histogram of the color channels separately\n",
    "    channel1_hist = np.histogram(img[:,:,0], bins=nbins)\n",
    "    channel2_hist = np.histogram(img[:,:,1], bins=nbins)\n",
    "    channel3_hist = np.histogram(img[:,:,2], bins=nbins)\n",
    "    # Concatenate the histograms into a single feature vector\n",
    "    hist_features = np.concatenate((channel1_hist[0], channel2_hist[0], channel3_hist[0]))\n",
    "    # Return the individual histograms, bin_centers and feature vector\n",
    "    return hist_features\n",
    "\n",
    "def color_hist_explore(img, axes):\n",
    "    axes[0].imshow(img)\n",
    "    cspace = 'RGB'\n",
    "    cimg = convert_color_from_rgb(img, cspace)\n",
    "    for i in range(3):\n",
    "        ax = axes[1+i]\n",
    "        channel_histogram = np.histogram(cimg[:,:,i], bins=16)\n",
    "        ax.hist(channel_histogram)\n",
    "        ax.set_title(cspace[i])\n",
    "\n",
    "# plot original and hog features for vehicle and non vehicle image\n",
    "f, (axes1, axes2) = plt.subplots(2, 4, figsize=(14,7))\n",
    "f.subplots_adjust(hspace = .4, wspace=.2)\n",
    "color_hist_explore(vehicle_imgs[70], axes1)\n",
    "color_hist_explore(nonvehicle_imgs[3], axes2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction for Training Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class FeatureExtractionParams(object):\n",
    "    def __init__(self, hog_color_space, hog_orientation, hog_pixels_per_cell, hog_cells_per_block, hog_channels, \n",
    "                 color_hist_bins=16, color_hist_cspace='RGB', spat_bins=32):\n",
    "        self.hog_color_space = hog_color_space \n",
    "        self.hog_orientation = hog_orientation\n",
    "        self.hog_pixels_per_cell = hog_pixels_per_cell\n",
    "        self.hog_cells_per_block = hog_cells_per_block\n",
    "        self.hog_channels=hog_channels\n",
    "        self.color_hist_bins = color_hist_bins\n",
    "        self.color_hist_cspace = color_hist_cspace\n",
    "        self.spat_bins = spat_bins\n",
    "        \n",
    "    def __str__(self):\n",
    "        return (\"FeatureExtractionParams[orientation=\" + str(self.hog_orientation) + \n",
    "            \", color space=\" + self.hog_color_space +\n",
    "            \", channels=\" + str(self.hog_channels) +\n",
    "            \", pixels per cell=\" + str(self.hog_pixels_per_cell) + \n",
    "            \", cells per block=\" + str(self.hog_cells_per_block) + \n",
    "            \", color histogram bins=\" + str(self.color_hist_bins) +\n",
    "            \", color histogram color space=\" + self.color_hist_cspace +\n",
    "            \", spatial bins=\" + str(self.spat_bins) +\n",
    "            \"]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def hog_features_for_image(img_channel, params: FeatureExtractionParams, feature_vec=True):\n",
    "    return hog(img_channel, \n",
    "        orientations=params.hog_orientation, \n",
    "        pixels_per_cell=(params.hog_pixels_per_cell, params.hog_pixels_per_cell),\n",
    "        cells_per_block=(params.hog_cells_per_block, params.hog_cells_per_block), \n",
    "        transform_sqrt=True, \n",
    "        visualise=False, \n",
    "        feature_vector=feature_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def features_for_image(img, params: FeatureExtractionParams, hog_channel_features=None):\n",
    "    features = []\n",
    "    if hog_channel_features is None:\n",
    "        img_for_hog = convert_color_from_rgb(img, params.hog_color_space)\n",
    "        for channel in params.hog_channels:\n",
    "            channel_features = hog_features_for_image(img_for_hog[:,:,channel], params)\n",
    "            features.append(channel_features)\n",
    "    else:\n",
    "        features.append(hog_channel_features)\n",
    "        \n",
    "    if params.color_hist_bins > 0:\n",
    "        img_for_hist = convert_color_from_rgb(img, params.color_hist_cspace)\n",
    "        features.append(color_hist(img_for_hist))\n",
    "        \n",
    "    if params.spat_bins > 0:\n",
    "        features.append(bin_spatial(img, (params.spat_bins, params.spat_bins)))\n",
    "        \n",
    "    return np.concatenate(features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Duplicated and adapted from udacity lesson code\n",
    "def features_for_image_list(imgs, params: FeatureExtractionParams):\n",
    "    features_list = []\n",
    "    for img in tqdm(imgs):\n",
    "        features_list.append(features_for_image(img, params))\n",
    "        # augment data set by flipping images\n",
    "        features_list.append(features_for_image(cv2.flip(img,1), params))\n",
    "    return features_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def build_dataset(params: FeatureExtractionParams):\n",
    "    vehicles_features = features_for_image_list(vehicle_imgs, params)\n",
    "    nonvehicles_features = features_for_image_list(nonvehicle_imgs, params)\n",
    "\n",
    "    # float64 for X is needed because StandardScaler expects it\n",
    "    X = np.vstack((vehicles_features, nonvehicles_features)).astype(np.float64)  \n",
    "    y = np.hstack((np.ones(len(vehicles_features)), np.zeros(len(nonvehicles_features))))\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Exploration for SVM Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_svm(X_train, y_train, X_test, y_test):\n",
    "    svc = LinearSVC()\n",
    "    svc.fit(X_train, y_train)\n",
    "    return svc, svc.score(X_test, y_test)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def explore_feature_extraction_params(params_list):\n",
    "    rand_state = 33\n",
    "    for params in params_list:\n",
    "        t_start = time.time()\n",
    "        X, y = build_dataset(params)\n",
    "        t_feature_extraction = time.time() - t_start\n",
    "\n",
    "        # split into train and test set\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            X, y, test_size=0.2, random_state=rand_state)\n",
    "\n",
    "        # scale features\n",
    "        X_scaler = StandardScaler().fit(X_train)\n",
    "        X_train = X_scaler.transform(X_train)\n",
    "        X_test = X_scaler.transform(X_test)\n",
    "        \n",
    "        t_train_start = time.time()\n",
    "        _, score = train_svm(X_train, y_train, X_test, y_test)\n",
    "        t_training = time.time() - t_train_start\n",
    "        \n",
    "        len_features = len(features_for_image(vehicle_imgs[0], params))\n",
    "\n",
    "        print(\"score=\" + \"{0:.2f}\".format(score) + \" for \" + str(params) +\n",
    "             \" in \" + \"{0:.1f}\".format(t_feature_extraction) + \" secs feature extraction and \" + \n",
    "             \"{0:.1f}\".format(t_training) + \" secs training with \" + str(len_features) + \" features/img\")\n",
    "\n",
    "explore_params = []\n",
    "explore_params.append(\n",
    "    FeatureExtractionParams(\n",
    "    hog_color_space = 'YCrCb',\n",
    "    hog_orientation = 8,\n",
    "    hog_pixels_per_cell = 8,\n",
    "    hog_cells_per_block = 2,\n",
    "    hog_channels=[0,1,2],\n",
    "    color_hist_bins=32,\n",
    "    color_hist_cspace='RGB',\n",
    "    spat_bins=0)\n",
    ")\n",
    "#explore_feature_extraction_params(explore_params)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|  No. |  Accuracy | Orientations | Color Space  | Channels  | Pixels per Cell  | Cells per Block | Color Histogram Bins | Time Feature Extraction | Time Training | Features/Img | Augmented? |\n",
    "|---|---|---|---|---|---|---|\n",
    "|  1 |  0.96 | 9  | HSV  |  2 | 8  | 2  | - | 22.5 | 5.2 | ? | n |\n",
    "|  2 |  0.98 | 9  | HSV  |  0,1,2 | 8  | 2  | - | 131.7 | 9 | ? | n |\n",
    "|  3 |  0.91 | 9  | HLS  |  2 | 8  | 2  | - | 39.6 | 7.2 | ? | n |\n",
    "|  4 |  0.98 | 9  | HLS  |  0,1,2 | 8  | 2  | - | 72.3 | 19.6 | ? | n |\n",
    "|  5 |  0.96 | 9  | YUV  |  0 | 8  | 2  | - | 36.6 | 5.1 | ? | n |\n",
    "|  6 |  0.98 | 9  | YUV  |  0,1,2 | 8  | 2  | - | 112.4 | 8.3 | ? | n |\n",
    "|  7 |  0.96 | 9  | YCrCb  |  0 | 8  | 2  | - | 35.6 | 5.1 | ? | n |\n",
    "|  8 |  0.98 | 9  | YCrCb  |  0,1,2 | 8  | 2  | - | 114.8 | 8.6 | ? | n |\n",
    "|  9 |  0.97 | 11  | YUV  |  0,1,2 | 16  | 2  | - | 49.3 | 4.5 | ? | n |\n",
    "|  10 |  0.96 | 11  | YUV  |  0 | 16  | 2  | - | 22.6 | 9.3 | ? | n |\n",
    "|  11 |  0.91 | 11  | HLS  |  0 | 16  | 2  | - | 18.1 | 13 | ? | n |\n",
    "|  12 |  0.92 | 16  | HLS  |  0 | 8  | 2  | - | 32.6 | 21 | ? | n |\n",
    "|  13 |  0.98 | 16  | HSV  |  1,2 | 8 | 2  | - | 105.7 | 31.9 | ? | n |\n",
    "|  14 |  0.96 | 8  | HSV  |  1,2 | 8 | 2  | - | 55.2 | 15.4 | ? | n |\n",
    "|  15 |  0.95 | 8  | YUV  |  0 | 8 | 2  | - | 24.9 | 10.3 | ? | n |\n",
    "|  16 |  0.95 | 8  | LUV  |  0 | 8 | 2  | - | 26.0 | 10.9 | ? | n |\n",
    "|  17 |  0.94 | 8  | LUV  |  0 | 8 | 2  | - | 102.4 | 40.8 | ? | y |\n",
    "|  18 |  0.96 | 8  | YUV  |  0 | 8 | 2  | 32 | ? | 21.0 | 1664 | y |\n",
    "|  19 |  0.99 | 8  | YUV  |  0,1,2 | 8 | 2  | 32 | 330.2 | 24.8| 4800 | y |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Linear SVM with chosen feature extraction parameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_extraction_params = FeatureExtractionParams(\n",
    "    hog_color_space = 'YCrCb',\n",
    "    hog_orientation = 8,\n",
    "    hog_pixels_per_cell = 8,\n",
    "    hog_cells_per_block = 2,\n",
    "    hog_channels=[0,1,2],\n",
    "    color_hist_bins=32,\n",
    "    color_hist_cspace='RGB',\n",
    "    spat_bins=0)\n",
    "\n",
    "print(\"feature count per image: \" + str(len(features_for_image(vehicle_imgs[0], feature_extraction_params))))\n",
    "\n",
    "X, y = build_dataset(feature_extraction_params)\n",
    "print(\"data set size (images): \" + str(len(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# split data set into train and test sub sets and also suffle it\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "print(\"feature count per image: \" + str(len(X[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# scale features\n",
    "X_scaler = StandardScaler().fit(X_train)\n",
    "X_train = X_scaler.transform(X_train)\n",
    "X_test = X_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Duplicated and adapted from udacity lesson code\n",
    "\n",
    "# Use a linear SVC \n",
    "svc = LinearSVC()\n",
    "# Check the training time for the SVC\n",
    "print('Feature vector length:', len(X_train[0]))\n",
    "t = time.time()\n",
    "svc.fit(X_train, y_train)\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 2), 'Seconds to train SVC...')\n",
    "# Check the score of the SVC\n",
    "print('Test Accuracy of SVC = ', round(svc.score(X_test, y_test), 4))\n",
    "# Check the prediction time for a single sample\n",
    "t=time.time()\n",
    "n_predict = 10\n",
    "print('My SVC predicts: ', svc.predict(X_test[0:n_predict]))\n",
    "print('For these',n_predict, 'labels: ', y_test[0:n_predict])\n",
    "t2 = time.time()\n",
    "print(round(t2-t, 5), 'Seconds to predict', n_predict,'labels with SVC')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function for Drawing Bounding Boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Duplicated from udacity lesson code\n",
    "def draw_boxes(img, bboxes, colors = [(0,255,0),(0,0,255)], thick=3):\n",
    "    # Make a copy of the image\n",
    "    imcopy = np.copy(img)\n",
    "    # Iterate through the bounding boxes\n",
    "    for idx,bbox in enumerate(bboxes):\n",
    "        # Draw a rectangle given bbox coordinates\n",
    "        cv2.rectangle(imcopy, bbox[0], bbox[1], colors[idx%len(colors)], thick)\n",
    "    # Return the image copy with boxes drawn\n",
    "    return imcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search Vehicles in Windows (HOG once per full image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Duplicated and adapted from udacity lesson code\n",
    "def find_cars(img, svc, feature_extraction_params, scale, ystart, ystop, all_windows=False):\n",
    "    \n",
    "    draw_img = np.copy(img)\n",
    "    #img = img.astype(np.float32)/255\n",
    "    \n",
    "    img_tosearch = img[ystart:ystop,:,:]\n",
    "    ctrans_tosearch = convert_color_from_rgb(img_tosearch, feature_extraction_params.hog_color_space)\n",
    "    \n",
    "    if scale != 1:\n",
    "        imshape = ctrans_tosearch.shape\n",
    "        ctrans_tosearch = cv2.resize(ctrans_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "        img_tosearch = cv2.resize(img_tosearch, (np.int(imshape[1]/scale), np.int(imshape[0]/scale)))\n",
    "    \n",
    "    hog_features = []\n",
    "    for channel in feature_extraction_params.hog_channels:\n",
    "        channel_features = hog_features_for_image(ctrans_tosearch[:,:,channel], \n",
    "                                                  feature_extraction_params, feature_vec=False)\n",
    "        hog_features.append(channel_features)\n",
    "\n",
    "    # Define blocks and steps as above\n",
    "    pix_per_cell = feature_extraction_params.hog_pixels_per_cell\n",
    "    cell_per_block = feature_extraction_params.hog_cells_per_block\n",
    "    nxblocks = (ctrans_tosearch.shape[1] // pix_per_cell) - cell_per_block + 1\n",
    "    nyblocks = (ctrans_tosearch.shape[0] // pix_per_cell) - cell_per_block + 1 \n",
    "    nfeat_per_block = feature_extraction_params.hog_orientation*cell_per_block**2\n",
    "    \n",
    "    # 64 was the orginal sampling rate, with 8 cells and 8 pix per cell\n",
    "    window = 64\n",
    "    nblocks_per_window = (window // pix_per_cell) - cell_per_block + 1\n",
    "    cells_per_step = 2  # Instead of overlap, define how many cells to step\n",
    "    nxsteps = (nxblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    nysteps = (nyblocks - nblocks_per_window) // cells_per_step + 1\n",
    "    \n",
    "    bboxes = []\n",
    "    for xb in range(nxsteps):\n",
    "        for yb in range(nysteps):\n",
    "            ypos = yb*cells_per_step\n",
    "            xpos = xb*cells_per_step\n",
    "            xleft = xpos*pix_per_cell\n",
    "            ytop = ypos*pix_per_cell\n",
    "            \n",
    "            # Extract HOG for this patch\n",
    "            hog_features_patch = []\n",
    "            for hog_channel_features in hog_features:\n",
    "                hog_features_patch.append(\n",
    "                    hog_channel_features[ypos:ypos+nblocks_per_window, xpos:xpos+nblocks_per_window].ravel())\n",
    "            hog_features_patch = np.concatenate(hog_features_patch)\n",
    "            \n",
    "            # Extract the image patch\n",
    "            subimg = cv2.resize(img_tosearch[ytop:ytop+window, xleft:xleft+window], (64,64))\n",
    "            \n",
    "            features_patch = features_for_image(subimg, feature_extraction_params, hog_features_patch)\n",
    "\n",
    "            # Scale features and make a prediction\n",
    "            # TODO float64 for scaler\n",
    "            test_features = X_scaler.transform(features_patch.reshape(1, -1))\n",
    "            #test_features = X_scaler.transform(np.hstack((hog_features, hist_features)).reshape(1, -1))    \n",
    "            #test_features = X_scaler.transform(np.hstack((shape_feat, hist_feat)).reshape(1, -1))    \n",
    "            \n",
    "            test_prediction = svc.predict(test_features)\n",
    "            \n",
    "            if test_prediction == 1 or all_windows:\n",
    "                xbox_left = np.int(xleft*scale)\n",
    "                ytop_draw = np.int(ytop*scale)\n",
    "                win_draw = np.int(window*scale)\n",
    "                bboxes.append(((xbox_left, ytop_draw+ystart),(xbox_left+win_draw,ytop_draw+win_draw+ystart)))\n",
    "                \n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def find_cars_multi_scale_layers(img, all_windows=False):\n",
    "    bboxes_scales = []\n",
    "    def append_bboxes(scale, ystart, ystop):\n",
    "        bboxes_scales.append(find_cars(img, svc, feature_extraction_params, scale, ystart, ystop, all_windows))\n",
    "    \n",
    "    # using a vertical overlap of 75 % (same as in find_cars for horicontal overlap)\n",
    "    append_bboxes(1.0, 400, 464)\n",
    "    append_bboxes(1.0, 416, 480)\n",
    "    append_bboxes(1.5, 400, 496)\n",
    "    append_bboxes(1.5, 424, 520)\n",
    "    append_bboxes(2.0, 400, 528)\n",
    "    append_bboxes(2.0, 432, 560)\n",
    "    append_bboxes(3.0, 400, 592)\n",
    "    append_bboxes(3.0, 448, 640)\n",
    "    \n",
    "    return bboxes_scales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "windows_layers = find_cars_multi_scale_layers(img_test, all_windows=True)\n",
    "windows_imgs = [draw_boxes(img_test, windows_layer) for windows_layer in windows_layers]\n",
    "plot_images_grid(windows_imgs, \n",
    "                 ['windows' + str(i) for i in range(len(windows_imgs))],\n",
    "                axis_on=True)\n",
    "\n",
    "for idx,img in enumerate(windows_imgs):\n",
    "    cv2.imwrite(\"./output_images/windows-\" + str(idx) + \".jpg\", cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "print(\"window count: \" + str(len(np.concatenate(windows_layers))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_cars_multi_scale(img):\n",
    "    bboxes_layers = find_cars_multi_scale_layers(img)\n",
    "    bboxes = []\n",
    "    for bboxes_layer in bboxes_layers:\n",
    "        for bbox in bboxes_layer:\n",
    "            bboxes.append(bbox)\n",
    "    return bboxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def bbox_img2(img):\n",
    "    bboxes = find_cars_multi_scale(img)\n",
    "    return draw_boxes(img, bboxes, colors=[(0,0,255)])                    \n",
    "\n",
    "imgs_test_bboxes = [bbox_img2(img) for img in imgs_test]\n",
    "plot_images_grid(imgs_test_bboxes)\n",
    "for idx,img in enumerate(imgs_test_bboxes):\n",
    "    cv2.imwrite(\"./output_images/imgs_test_bboxes_\" + str(idx) + \".jpg\", cv2.cvtColor(img, cv2.COLOR_RGB2BGR))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict Again on Bounding Boxes found in a Test Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def write_bboxes_as_files(img):\n",
    "    bboxes = find_cars_multi_scale(img)\n",
    "    for idx,bbox in enumerate(bboxes):\n",
    "        y_min = bbox[0][1]\n",
    "        y_max = bbox[1][1]\n",
    "        x_min = bbox[0][0]\n",
    "        x_max = bbox[1][0]\n",
    "        img_bbox = cv2.resize(img[y_min:y_max,x_min:x_max], (64,64))\n",
    "        file = \"./output_images/bboxes_invalid_\" + str(idx) + \".jpg\"\n",
    "        cv2.imwrite(file, cv2.cvtColor(img_bbox, cv2.COLOR_RGB2BGR))\n",
    "        \n",
    "        features_bbox = features_for_image(img_bbox, feature_extraction_params)\n",
    "        test_features = X_scaler.transform(features_bbox.reshape(1, -1))\n",
    "        test_prediction = svc.predict(test_features)\n",
    "        print(file + \": predict=\" + str(test_prediction))\n",
    "        \n",
    "#write_bboxes_as_files(imgs_test[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering False Positives using Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Duplicated and adapted from udacity lesson code\n",
    "\n",
    "def heatmap_add(heatmap, bbox_list):\n",
    "    # Iterate through list of bboxes\n",
    "    for box in bbox_list:\n",
    "        # Add += 1 for all pixels inside each bbox\n",
    "        # Assuming each \"box\" takes the form ((x1, y1), (x2, y2))\n",
    "        heatmap[box[0][1]:box[1][1], box[0][0]:box[1][0]] += 1\n",
    "\n",
    "    # Return updated heatmap\n",
    "    return heatmap# Iterate through list of bboxes\n",
    "    \n",
    "def heatmap_apply_threshold(heatmap, threshold):\n",
    "    # Zero out pixels below the threshold\n",
    "    heatmap[heatmap <= threshold] = 0\n",
    "    # Return thresholded map\n",
    "    return heatmap\n",
    "\n",
    "def draw_labeled_bboxes(img, labels):\n",
    "    # Iterate through all detected cars\n",
    "    for car_number in range(1, labels[1]+1):\n",
    "        # Find pixels with each car_number label value\n",
    "        nonzero = (labels[0] == car_number).nonzero()\n",
    "        # Identify x and y values of those pixels\n",
    "        nonzeroy = np.array(nonzero[0])\n",
    "        nonzerox = np.array(nonzero[1])\n",
    "        # Define a bounding box based on min/max x and y\n",
    "        bbox = ((np.min(nonzerox), np.min(nonzeroy)), (np.max(nonzerox), np.max(nonzeroy)))\n",
    "        # Draw the box on the image\n",
    "        cv2.rectangle(img, bbox[0], bbox[1], (0,0,255), 6)\n",
    "    # Return the image\n",
    "    return img\n",
    "\n",
    "# Add heat to each box in box list\n",
    "test_heatmap = np.zeros_like(img_test[:,:,0]).astype(np.float)\n",
    "test_bboxes = find_cars_multi_scale(img_test)\n",
    "test_heatmap = heatmap_add(test_heatmap, test_bboxes)\n",
    "    \n",
    "# Apply threshold to help remove false positives\n",
    "test_heat = heatmap_apply_threshold(test_heatmap, 3)\n",
    "\n",
    "# Find final boxes from heatmap using label function\n",
    "labels = label(test_heatmap)\n",
    "test_img_labels = draw_labeled_bboxes(np.copy(img_test), labels)\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.subplot(121)\n",
    "plt.imshow(test_img_labels)\n",
    "plt.title('Car Positions')\n",
    "plt.subplot(122)\n",
    "plt.imshow(test_heatmap, cmap='hot')\n",
    "plt.title('Heat Map')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "heat_history_max_len = 2\n",
    "heat_history = collections.deque([], heat_history_max_len)\n",
    "def process_video_image(img):\n",
    "    bboxes = find_cars_multi_scale(img)\n",
    "    \n",
    "    # without heatmap\n",
    "    # return draw_boxes(img, vehicle_bboxes, color=(0, 0, 255), thick=6)                    \n",
    "\n",
    "    # TODO Visualize the heatmap when displaying    \n",
    "    #heatmap = np.clip(heat, 0, 255)\n",
    "    \n",
    "    heat = np.zeros_like(img[:,:,0])\n",
    "    heat = heatmap_add(heat, bboxes)\n",
    "    heat_new = np.copy(heat)\n",
    "    for heat_past in heat_history:\n",
    "        heat += heat_past\n",
    "    heat_history.append(heat_new)\n",
    "    heat = heatmap_apply_threshold(heat, 4)\n",
    "    labels = label(heat)\n",
    "    return draw_labeled_bboxes(img, labels)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clip = VideoFileClip(\"test_video.mp4\")\n",
    "clip_cut = clip #clip.subclip(19, 24)\n",
    "clip_augmented = clip_cut.fl_image(process_video_image)\n",
    "clip_augmented.write_videofile(\"test_video_result.mp4\", audio=False, progress_bar=True)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(\"test_video_result.mp4\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project Video Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "clip = VideoFileClip(\"project_video.mp4\")\n",
    "clip_cut = clip #clip.subclip(19, 24)\n",
    "clip_augmented = clip_cut.fl_image(process_video_image)\n",
    "clip_augmented.write_videofile(\"project_video_result.mp4\", audio=False, progress_bar=True)\n",
    "\n",
    "HTML(\"\"\"\n",
    "<video width=\"640\" height=\"360\" controls>\n",
    "  <source src=\"{0}\">\n",
    "</video>\n",
    "\"\"\".format(\"project_video_result.mp4\"))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
